apiVersion: v1
kind: Namespace
metadata:
  name: violate-policy
---
apiVersion: v1
kind: Pod
metadata:
  name: violating-pod
  namespace: violate-policy
  labels:
    purpose: violate-policy
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
    # Missing resources limits and requests
    resources: {}
    # Running as root
    securityContext:
      runAsUser: 0
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
  namespace: violate-policy
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  # Allowing all traffic, does not restrict anything
  ingress:
  - {}
  egress:
  - {}
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: allow-all
  namespace: violate-policy
subjects:
- kind: ServiceAccount
  name: default
  namespace: violate-policy
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: allow-all
subjects:
- kind: User
  name: admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
# Assume the Kubernetes API server configuration file is named kube-apiserver.yaml and is typically not part of the Pod manifest.
# The file should be placed on the master node, but here is how it might look without audit logging enabled.
# /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver:v1.18.0
    command:
    - kube-apiserver
    # Audit logging is not configured here
    - --audit-log-path=
    - --audit-policy-file=
